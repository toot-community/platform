# Sidekiq (Mastodon queue workers) autoscaling: requirements and implementation

This document captures the **requirements** and **implementation details** for autoscaling Mastodon Sidekiq worker pods with **KEDA** in this repo.

## Requirements (what must be true)

### 1) Queue topology

- The `scheduler` queue **must run in exactly one Sidekiq process**.
  - In this repo that means: a dedicated worker group with `queues: ["scheduler"]` and `replicas: 1`.
  - The chart will **refuse to render** a KEDA ScaledObject for any worker that includes `scheduler`.
- Avoid queue starvation: queues that have different urgency should **not** share the same Sidekiq process if that causes delays.
- Prefer “single ownership” of a queue: each queue should be consumed by **exactly one** worker group (unless you explicitly want duplicate consumption).

### 2) DB connection budget

- Every Sidekiq thread needs a DB connection (ActiveRecord pool sizing).
- This chart sets `DB_POOL` to the worker `concurrency` for Sidekiq pods.
- `maxReplicaCount` across all worker groups should be chosen to stay within your Postgres/PgBouncer budget.

### 3) Observability prerequisites

KEDA scaling is only supported when:

- `observability.metrics.enabled: true`
- `observability.metrics.podMonitor.enabled: true`
- Sidekiq detailed metrics are enabled (we default `observability.metrics.sidekiq.detailed: true`)

## Recommended topology in this repo

### toot.community (production)

Use the moderate split:

1. `default` → `default`
2. `federation` → `push`, `ingress`, `fasp`
3. `pull` → `pull`
4. `mailers` → `mailers`
5. `scheduler` → `scheduler` (fixed `replicas: 1`, **no autoscaling**)

Configured in `manifests/applications/mastodon/overlays/toot.community/helm-values/mastodon.yaml`.

### microblog.network (testing)

Use the minimal split (current default topology) and autoscale `generic`:

- `generic` → `default`, `mailers`, `push`, `ingress`, `pull`, `fasp`
- `scheduler` → `scheduler` (fixed `replicas: 1`)

Configured in `manifests/applications/mastodon/overlays/microblog.network/helm-values/mastodon.yaml`.

## Scaling signals (what we scale on)

Each autoscaled worker group gets one KEDA ScaledObject with Prometheus triggers:

1. **Queue backlog** (`sidekiq_queue_backlog`) — `metricType: AverageValue` (jobs-per-replica)
2. **Queue latency** (`sidekiq_queue_latency_seconds`) — `metricType: Value` (absolute seconds)

We use `max(max_over_time(...[window]))` and end queries with `or vector(0)` to guarantee a single value even when metrics are absent.

The query is scoped by `job="<namespace>/<podmonitor-name>"` where the PodMonitor name is:

- `<release>-sidekiq-<worker>`

This selector is generated by `mastodon.kedaSidekiqSelector` in `charts/mastodon/templates/_helpers.tpl`.

## How to configure (values)

### Global switch (chart-wide)

- Enable Sidekiq KEDA rendering:
  - `sidekiq.autoscaling.keda.enabled: true`
- Optional: override Prometheus endpoint:
  - `sidekiq.autoscaling.keda.prometheus.serverAddress: ...`
  - If unset, the chart defaults to `web.autoscaling.keda.prometheus.serverAddress`.

### Per worker group

Enable autoscaling on specific workers:

- `sidekiq.workers[].autoscaling.keda.enabled: true`
- Set bounds:
  - `minReplicaCount`
  - `maxReplicaCount`

### Presets (recommended)

The chart ships presets under:

- `sidekiq.autoscaling.keda.prometheus.presets`

Workers can select a preset with:

- `sidekiq.workers[].autoscaling.keda.preset`

If `preset` is omitted, the worker `name` is used as the preset key (e.g., worker `name: pull` → preset `pull`).

## Implementation details (where this lives)

- Sidekiq deployments:
  - `charts/mastodon/templates/deployment-sidekiq.yaml`
  - When a worker is KEDA-autoscaled, the Deployment `replicas:` field is omitted to avoid fighting the HPA.
- Sidekiq ScaledObjects:
  - `charts/mastodon/templates/scaledobject-sidekiq.yaml`
- Default presets and global config:
  - `charts/mastodon/values.yaml`

## Tuning guidelines

- Start with conservative `maxReplicaCount` values and raise only after validating DB/Redis headroom.
- Keep `mailers` `maxReplicaCount` low to avoid SMTP provider throttling.
- If “less urgent” queues (like `pull`) should not influence urgent work, split them into their own worker group (as in toot.community).

