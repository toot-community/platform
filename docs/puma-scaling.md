# Puma (Mastodon web) autoscaling: rationale and heuristics

This document explains how we scale Mastodon **web** pods (Puma) with KEDA and why we chose these specific signals and parameters. The goal is to preserve institutional knowledge about **what** we scale on and **why**.

## Summary

We use **two Prometheus triggers** for KEDA:

1. **Busy fraction (thread headroom)** — early scale-up signal
2. **Request backlog** — panic scale-up signal

KEDA feeds these into the HPA; HPA uses the metric that asks for the **largest** replica count. We configure aggressive scale-up and conservative scale-down behavior to react quickly to bursts and avoid oscillation.

## Why not scale on `running_threads`

Puma exposes several thread pool metrics. The key ones for autoscaling:

- `*_puma_running_threads`: threads spawned, not necessarily busy.
- `*_puma_thread_pool_capacity`: **available** threads at current scale (includes not-yet-spawned capacity when `min_threads < max_threads`).
- `*_puma_max_threads`: configured maximum threads.
- `*_puma_request_backlog`: requests waiting for a Puma thread.

`running_threads` is often **flat** (especially when `min_threads == max_threads`) and doesn’t represent saturation. It is therefore a poor primary scaling signal.

## Primary early signal: busy fraction (thread headroom)

We derive a utilization proxy from `pool_capacity` and `max_threads`:

```
busy_fraction ≈ 1 - (pool_capacity / max_threads)
```

This works because `pool_capacity` is Puma’s available thread count. As utilization rises, `pool_capacity` falls. The ratio produces a 0–1 fraction that rises **before** backlog becomes large, giving earlier scale-up.

### Why `max_over_time` with a subquery

We wrap the expression with `max_over_time(...[2m:10s])`. This makes the scaler “remember” short spikes that could be missed between scrapes or HPA syncs. If a burst happens between polls, the maximum over the last 2 minutes still captures it.

## Secondary panic signal: request backlog

Backlog represents requests waiting for a Puma thread. It is a good **magnitude** signal during overloads or DDoS-like spikes. Backlog can jump sharply, and the metric provides a way to request a larger replica jump than a 0–1 fraction can.

We also apply `max_over_time(...[1m:10s])` here to capture short spikes.

### Backlog caveats

Puma backlog is not the same as OS socket backlog. Depending on Puma’s acceptance/backpressure behavior, backlog may under- or over-represent upstream queuing. That’s why it is **not** our only signal; it is paired with the busy-fraction headroom metric.

## KEDA/HPA configuration in this repo

### Template

- `charts/mastodon/templates/scaledobject-web.yaml`

This ScaledObject is only created when:

- `web.autoscaling.keda.enabled` is true
- `observability.metrics.enabled` and `observability.metrics.podMonitor.enabled` are true

### Queries

Busy fraction (early):

```
max_over_time((
  1 - (
    sum(ruby_puma_thread_pool_capacity{job="<namespace>/<release>-web"})
    / clamp_min(sum(ruby_puma_max_threads{job="<namespace>/<release>-web"}), 1)
  )
)[2m:10s])
```

Backlog (panic):

```
max_over_time(
  sum(ruby_puma_request_backlog{job="<namespace>/<release>-web"})
[1m:10s])
```

The `job=...` selector is generated by `mastodon.kedaWebPumaSelector` in `charts/mastodon/templates/_helpers.tpl` and matches the PodMonitor naming convention.

### Thresholds and activation thresholds

Defined in `charts/mastodon/values.yaml`:

- Busy fraction
  - `threshold: 0.70`
  - `activationThreshold: 0.60`
- Backlog
  - `threshold: 40`
  - `activationThreshold: 10`

**Why these?**

- `0.70` is a conservative “early warning” level: we still have headroom but are trending toward saturation.
- `40 backlog per pod` (AverageValue) allows large jumps during spikes and can be tuned depending on how much burst capacity we want vs. cost.
- Activation thresholds prevent the triggers from becoming active on minor, short-lived noise.

### Polling interval and cooldown

- `pollingInterval: 10` seconds (KEDA)
- `cooldownPeriod: 300` seconds

KEDA polls the metrics frequently enough to react to short bursts. For **scale-down**, we rely more on HPA behavior than on cooldown.

### HPA behavior (fast up, slow down)

Defined in the template:

- Scale-up:
  - `stabilizationWindowSeconds: 0`
  - `selectPolicy: Max`
  - Up to **200%** or **+10 pods** per 60s (whichever is larger)
- Scale-down:
  - `stabilizationWindowSeconds: 900` (15 minutes)
  - `selectPolicy: Min`
  - Down at most **10% per minute**

This is intentional: we want to **catch bursts quickly** but **release capacity slowly** to avoid thrash.

## Why minReplicaCount is set to 4 for toot.community

In `manifests/applications/mastodon/overlays/toot.community/helm-values/mastodon.yaml` we set:

- `web.replicas: 4`
- `web.autoscaling.keda.minReplicaCount: 4`

Reasons for a non-zero (and relatively high) minimum:

- Avoid cold starts and latency spikes during normal traffic.
- Maintain baseline availability even during brief quiet periods.
- Keep enough pods to satisfy the Pod Disruption Budget during rollouts.
- We do **not** want scale-to-zero for the web tier.

By setting min replicas to the current steady-state (`replicas: 4`), we ensure KEDA never scales below the expected baseline and that HPA scaling is purely additive during load.

## Operational caveats and tuning

- **Null metrics**: the Prometheus scaler is set to `ignoreNullValues: "true"` so missing series do not scale to zero unexpectedly.
- **DB/Redis pressure**: scaling web can shift load to the database and Redis. `maxReplicaCount` should reflect what the backend can tolerate.

## Where to tune

- `charts/mastodon/values.yaml` (defaults)
- `manifests/applications/mastodon/overlays/toot.community/helm-values/mastodon.yaml` (instance overrides)
- `charts/mastodon/templates/scaledobject-web.yaml` (template logic)

## Future improvements

If we ever add a request-queueing-time metric (e.g., from `X-Request-Start`), it can become an even earlier signal than backlog. For now, busy fraction + backlog provides a good balance of **early warning** and **panic scaling**.
